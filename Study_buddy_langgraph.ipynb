{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72a2405e",
   "metadata": {},
   "source": [
    "<h1 align='center'>Study Buddy</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2979285e",
   "metadata": {},
   "source": [
    "## Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbd443bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utkarsh\\Documents\\Projects\\Study-Buddy\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "import weaviate\n",
    "from weaviate.classes.config import Property, DataType, Configure\n",
    "\n",
    "from pinecone import Pinecone\n",
    "import config\n",
    "\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fa6e24",
   "metadata": {},
   "source": [
    "## Connecting to the DBs and servers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af10f5b8",
   "metadata": {},
   "source": [
    "#### Loading User Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32ad4e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_PROFILE_FILE = \"user_profile.json\"\n",
    "\n",
    "def load_user_profile():\n",
    "    if not os.path.exists(USER_PROFILE_FILE):\n",
    "        return {\"name\": None}\n",
    "    try:\n",
    "        with open(USER_PROFILE_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    except json.JSONDecodeError:\n",
    "        return {\"name\": None}\n",
    "    \n",
    "def save_user_profile(profile_data):\n",
    "    with open(USER_PROFILE_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(profile_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "236647b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Weaviate:  True\n"
     ]
    }
   ],
   "source": [
    "# ===== A. Weaviate (for Episodic Memory) =====\n",
    "\n",
    "try:\n",
    "    vdb_client = weaviate.connect_to_local()\n",
    "    print(\"Connected to Weaviate: \", vdb_client.is_ready())\n",
    "except Exception as e:\n",
    "    print(f\"Could not connect to Weaviate. Is it running? \\nError: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da5dda1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vdb_client.collections.delete(\"episodic_memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee59fe05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Pinecone Index: study-buddy-768\n",
      "{'dimension': 768,\n",
      " 'index_fullness': 0.0,\n",
      " 'metric': 'cosine',\n",
      " 'namespaces': {'': {'vector_count': 71}},\n",
      " 'total_vector_count': 71,\n",
      " 'vector_type': 'dense'}\n"
     ]
    }
   ],
   "source": [
    "# ===== B. Pinecone (for Semantic Memory) =====\n",
    "\n",
    "pc = Pinecone(api_key=config.PINECONE_API_KEY)\n",
    "pinecone_index = pc.Index(config.PINECONE_INDEX_NAME)\n",
    "print(f\"Connected to Pinecone Index: {config.PINECONE_INDEX_NAME}\")\n",
    "print(pinecone_index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee5724b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatOllama initialized with model: gpt-oss:20b-cloud\n",
      "OllamaEmbeddings initialized with model: nomic-embed-text:latest\n"
     ]
    }
   ],
   "source": [
    "# ===== C. NVIDIA Client (for Semantic Embeddings and LLM) =====\n",
    "\n",
    "OLLAMA_EMBED_MODEL = \"nomic-embed-text:latest\"\n",
    "OLLAMA_CHAT_MODEL = \"gpt-oss:20b-cloud\"\n",
    "OLLAMA_API_ENDPOINT = \"http://host.docker.internal:11434\"\n",
    "\n",
    "# This LLM is for *chat and reflection*\n",
    "llm = ChatOllama(model=OLLAMA_CHAT_MODEL, temperature=0.7)\n",
    "print(f\"ChatOllama initialized with model: {OLLAMA_CHAT_MODEL}\")\n",
    "\n",
    "# This embedder is for *querying Pinecone*\n",
    "embedding_model = OllamaEmbeddings(model=OLLAMA_EMBED_MODEL)\n",
    "print(f\"OllamaEmbeddings initialized with model: {OLLAMA_EMBED_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ffdd3b",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2578764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== \"STUDY BUDDY\" REFLECTION PROMPT =====\n",
    "\n",
    "reflection_prompt_template = \"\"\"\n",
    "You are an AI assistant analyzing a study session with a student. \n",
    "Your task is to create a memory that will help you be a better \"STUDY-BUDDY\" in the future.\n",
    "\n",
    "Review the conversation and create a memory reflection following these rules:\n",
    "1. For any field where you don't have enough information, use \"N/A\".\n",
    "2. Be extremely concise.\n",
    "3. Focus *only* on the user's learning, knowledge gaps, and preferences.\n",
    "\n",
    "Output valid JSON in exactly this format:\n",
    "{{\n",
    "    \"context_tags\": [              // 2-4 keywords about the general topic (e.g., \"calculus\", \"keynesian_economics\")\n",
    "        string,\n",
    "        ...\n",
    "    ],\n",
    "    \"conversation_summary\": string, // One sentence describing what the conversation accomplished (e.g., \"User was quizzed on Chapter 2 and struggled with 'opportunity cost'.\")\n",
    "    \"identified_weak_topics\": [    // Specific concepts or topics the user was confused about or answered incorrectly.\n",
    "        string,                    // e.g., \"Opportunity Cost\", \"Derivative of sin(x)\"\n",
    "        ...\n",
    "    ],\n",
    "    \"identified_strong_topics\": [  // Specific concepts the user understood well or answered correctly.\n",
    "        string,\n",
    "        ...\n",
    "    ],\n",
    "    \"user_learning_preference\": string, // Any observed preferences (e.g., \"Prefers multiple-choice questions\", \"Responds well to mnemonics\", \"N/A\"),\n",
    "    \"user_name\": string // e.g., \"Utkarsh\" or \"N/A\" if not mentioned or confirmed\n",
    "}}\n",
    "\n",
    "Do not include any text outside the JSON object in your response.\n",
    "\n",
    "Here is the prior conversation:\n",
    "\n",
    "{conversation}\n",
    "\"\"\"\n",
    "\n",
    "reflection_prompt = ChatPromptTemplate.from_template(reflection_prompt_template)\n",
    "reflect = reflection_prompt | llm | JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a158213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== FORMAT CONVERSATION HELPER =====\n",
    "\n",
    "def format_conversation(messages):\n",
    "    conversation = []\n",
    "    for message in messages:\n",
    "        # This robustly skips any non-message objects that might be in the list\n",
    "        if not hasattr(message, 'type') or not hasattr(message, 'content'):\n",
    "            continue\n",
    "\n",
    "        # Skip the initial large system/semantic prompts for a cleaner reflection\n",
    "        if message.type == \"system\":\n",
    "            continue\n",
    "        if \"Use this grounded context\" in message.content:\n",
    "            continue\n",
    "            \n",
    "        conversation.append(f\"{message.type.upper()}: {message.content}\")\n",
    "    \n",
    "    return \"\\n\".join(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7825368e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===== RETRIEVAL FUNCTIONS =====\n",
    "\n",
    "# # === A. EPISODIC RECALL ===\n",
    "# def episodic_recall(query, vdb_client):\n",
    "#     try:\n",
    "#         episodic_memory = vdb_client.collections.get(\"episodic_memory\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error getting episodic_memory collection: {e}\")\n",
    "#         print(\"Attempting to create 'episodic_memory' collection...\")\n",
    "#         # If the collection doesn't exist, create it.\n",
    "#         # This is a simple setup. You might want to configure vectors explicitly.\n",
    "#         episodic_memory = vdb_client.collections.create(name=\"episodic_memory\")\n",
    "        \n",
    "#         # Add a dummy entry so the first query doesn't fail\n",
    "#         episodic_memory.data.insert({\n",
    "#             \"conversation\": \"USER: Hello\\nAI: Hi! Welcome to STUDY-BUDDY.\",\n",
    "#             \"context_tags\": [\"initial_setup\"],\n",
    "#             \"conversation_summary\": \"Initial conversation setup.\",\n",
    "#             \"identified_weak_topics\": [\"N/A\"],\n",
    "#             \"identified_strong_topics\": [\"N/A\"],\n",
    "#             \"user_learning_preference\": \"N/A\"\n",
    "#         })\n",
    "#         print(\"Collection 'episodic_memory' created with a dummy entry.\")\n",
    "\n",
    "#     memory = episodic_memory.query.hybrid(\n",
    "#         query=query,\n",
    "#         alpha=0.5,\n",
    "#         limit=1,\n",
    "#     )\n",
    "    \n",
    "#     # Handle case where memory is empty\n",
    "#     if not memory.objects:\n",
    "#         # This can happen on the very first run if creation fails or is slow\n",
    "#         print(\"No episodic memory found. Using default.\")\n",
    "#         return {\n",
    "#             \"conversation\": \"N/A\",\n",
    "#             \"identified_weak_topics\": [\"N/A\"],\n",
    "#             \"identified_strong_topics\": [\"N/A\"],\n",
    "#             \"user_learning_preference\": \"N/A\"\n",
    "#         }\n",
    "    \n",
    "#     return memory.objects[0].properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ac61153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== RETRIEVAL FUNCTIONS =====\n",
    "\n",
    "# === A. EPISODIC RECALL ===\n",
    "def episodic_recall(query, vdb_client):\n",
    "    \n",
    "    COLLECTION_NAME = \"episodic_memory\"\n",
    "\n",
    "    # 1. Check if the collection exists\n",
    "    if not vdb_client.collections.exists(COLLECTION_NAME):\n",
    "        print(f\"Collection '{COLLECTION_NAME}' not found. Creating it now...\")\n",
    "        try:\n",
    "            # 2. If not, create it\n",
    "            episodic_memory_collection = vdb_client.collections.create(\n",
    "                name=COLLECTION_NAME,\n",
    "                description=\"STUDY-BUDDY Episodic Memory\",\n",
    "                \n",
    "                # 3. Configure the NVIDIA vectorizer\n",
    "                vectorizer_config=[\n",
    "                    Configure.NamedVectors.text2vec_ollama(\n",
    "                        name=\"conversation_summary_vector\",\n",
    "                        source_properties=[\"conversation_summary\"],\n",
    "                        api_endpoint=OLLAMA_API_ENDPOINT,\n",
    "                        model=OLLAMA_EMBED_MODEL,\n",
    "                    )\n",
    "                ],\n",
    "                \n",
    "                # 4. Define the properties to match our \"STUDY-BUDDY\" reflection prompt\n",
    "                properties=[\n",
    "                    Property(name=\"conversation\", data_type=DataType.TEXT),\n",
    "                    Property(name=\"context_tags\", data_type=DataType.TEXT_ARRAY),\n",
    "                    Property(name=\"conversation_summary\", data_type=DataType.TEXT),\n",
    "                    Property(name=\"identified_weak_topics\", data_type=DataType.TEXT_ARRAY),\n",
    "                    Property(name=\"identified_strong_topics\", data_type=DataType.TEXT_ARRAY),\n",
    "                    Property(name=\"user_learning_preference\", data_type=DataType.TEXT),\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            print(f\"Collection '{COLLECTION_NAME}' created successfully.\")\n",
    "            \n",
    "            # 5. Add a dummy entry so the *first* query doesn't fail\n",
    "            episodic_memory_collection.data.insert({\n",
    "                \"conversation\": \"USER: Hello\\nAI: Hi! Welcome to STUDY-BUDDY.\",\n",
    "                \"context_tags\": [\"initial_setup\"],\n",
    "                \"conversation_summary\": \"Initial conversation setup.\",\n",
    "                \"identified_weak_topics\": [],\n",
    "                \"identified_strong_topics\": [],\n",
    "                \"user_learning_preference\": \"N/A\"\n",
    "            })\n",
    "            print(\"Added dummy entry to episodic memory.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"CRITICAL: Failed to create Weaviate collection: {e}\")\n",
    "            print(\"Check that Weaviate is running and Ollama is accessible at {OLLAMA_API_ENDPOINT}\")\n",
    "            return {\n",
    "                \"conversation_summary\": \"N/A (Collection Creation Failed)\",\n",
    "                \"identified_weak_topics\": [],\n",
    "                \"identified_strong_topics\": [],\n",
    "                \"user_learning_preference\": \"N/A\"\n",
    "            }\n",
    "\n",
    "    # 6. Now that we know it exists, get the collection\n",
    "    episodic_memory = vdb_client.collections.get(COLLECTION_NAME)\n",
    "\n",
    "    # 7. Perform the hybrid query\n",
    "    memory = episodic_memory.query.hybrid(\n",
    "        query=query,\n",
    "        alpha=0.5, # Balances keyword and vector search\n",
    "        limit=1,\n",
    "    )\n",
    "    \n",
    "    # Handle case where memory is empty (should only be the dummy entry at first)\n",
    "    if not memory.objects:\n",
    "        print(\"No episodic memory found. Using default.\")\n",
    "        return {\n",
    "            \"conversation_summary\": \"N/A\",\n",
    "            \"identified_weak_topics\": [],\n",
    "            \"identified_strong_topics\": [],\n",
    "            \"user_learning_preference\": \"N/A\"\n",
    "        }\n",
    "    \n",
    "    return memory.objects[0].properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef708d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === B. EMBEDDING FUNCTION (Pinecone) ===\n",
    "\n",
    "def get_embeddings(texts, client=embedding_model):\n",
    "    # Use LangChain's batch embedding method\n",
    "    return client.embed_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fd658b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === C. SEMANTIC COLLECTION RETRIEVAL (from Pinecone) ===\n",
    "\n",
    "def pinecone_semantic_recall(query, pinecone_index, embedding_model):\n",
    "    \n",
    "    # 1. Create query embedding\n",
    "    query_vector = embedding_model.embed_query(query)\n",
    "    \n",
    "    # 2. Query Pinecone\n",
    "    query_results = pinecone_index.query(\n",
    "        vector=query_vector,\n",
    "        top_k=5, # Retrieve top 5 relevant chunks\n",
    "        include_metadata=True\n",
    "    )\n",
    "    \n",
    "    # 3. Format the results\n",
    "    combined_text = \"\"\n",
    "    for i, match in enumerate(query_results['matches']):\n",
    "        if 'text_snippet' in match['metadata']:\n",
    "            text = match['metadata']['text_snippet']\n",
    "        else:\n",
    "            # Fallback if text_snippet isn't in metadata\n",
    "            text = match['metadata'].get('text', 'No text snippet available.')\n",
    "\n",
    "        combined_text += f\"\\n--- Context Chunk {i+1} (from Page {match['metadata'].get('page_number', 'N/A')}) ---\\n\"\n",
    "        combined_text += text.strip()\n",
    "    \n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcf490f",
   "metadata": {},
   "source": [
    "## Defining LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "394d5e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== LANGGRAPH STATE =====\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: list\n",
    "    semantic_memory: str\n",
    "\n",
    "    # We will aggregate weak/strong topics here\n",
    "    procedural_memory: str\n",
    "    prior_conversations: list\n",
    "    weak_topics: list\n",
    "    strong_topics: list\n",
    "    learning_preferences: list\n",
    "    end: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b519f8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== LANGGRAPH NODES =====\n",
    "\n",
    "# === NODE 1: POPULATE STATE ===\n",
    "\n",
    "def populate_state(state: State):\n",
    "\n",
    "    initial_messages = []\n",
    "\n",
    "    first_query = input(\"User: \")\n",
    "    first_message = HumanMessage(first_query)\n",
    "\n",
    "    # --- Load User Profile ---\n",
    "    user_profile = load_user_profile()\n",
    "    user_name = user_profile.get(\"name\") # Will be None if not set\n",
    "\n",
    "    if user_name:\n",
    "        greeting_instruction = f\"\"\"\n",
    "        ALWAYS greet the user by their name, \"{user_name}\".\n",
    "        Example: \"Hello {user_name}! What can we work on today?\"\n",
    "        \"\"\"\n",
    "    else:\n",
    "        greeting_instruction = \"\"\"\n",
    "        You do not know the user's name. You can ask for it.\n",
    "        \"\"\"\n",
    "    \n",
    "    # --- Procedural Memory ---\n",
    "    try:\n",
    "        with open(\"./procedural_memory.txt\", \"r\", encoding=\"utf-8\") as content:\n",
    "            procedural_memory = content.read()\n",
    "    except FileNotFoundError:\n",
    "        print(\"procedural_memory.txt not found. Using default.\")\n",
    "        procedural_memory = \"1. Greet the user and ask what topic they want to study.\"\n",
    "        # Create the file so the update node doesn't fail\n",
    "        with open(\"./procedural_memory.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(procedural_memory)\n",
    "\n",
    "    # --- Episodic Memory  ---\n",
    "    episodic_memory = episodic_recall(first_query, vdb_client)\n",
    "    \n",
    "    # Use .get() for safety, providing defaults\n",
    "    prior_conversations = [episodic_memory.get('conversation_summary', 'N/A')]\n",
    "    weak_topics = episodic_memory.get('identified_weak_topics', [])\n",
    "    strong_topics = episodic_memory.get('identified_strong_topics', [])\n",
    "    learning_preferences = [episodic_memory.get('user_learning_preference', 'N/A')]\n",
    "    \n",
    "    # Ensure they are lists\n",
    "    if not isinstance(weak_topics, list): weak_topics = [weak_topics]\n",
    "    if not isinstance(strong_topics, list): strong_topics = [strong_topics]\n",
    "\n",
    "    episodic_prompt = f\"\"\"You are \"STUDY-BUDDY\", a helpful AI Assistant for learning and revision.\n",
    "    \n",
    "    {greeting_instruction}\n",
    "\n",
    "    Here is a summary of your last relevant conversation with this user:\n",
    "    - {prior_conversations[0]}\n",
    "    \n",
    "    Based on past sessions, here are topics the user finds difficult:\n",
    "    - {', '.join(weak_topics) if weak_topics else 'N/A'}\n",
    "    \n",
    "    Here are topics the user understands well:\n",
    "    - {', '.join(strong_topics) if strong_topics else 'N/A'}\n",
    "    \n",
    "    Use these memories to personalize your response.\n",
    "    \n",
    "    Additionally, follow these procedural guidelines: \n",
    "    {procedural_memory}\n",
    "    \"\"\"\n",
    "    system_prompt = SystemMessage(episodic_prompt)\n",
    "\n",
    "    # --- Semantic Memory ---\n",
    "    semantic_memory_retrieval = pinecone_semantic_recall(first_query, pinecone_index, embedding_model)\n",
    "    \n",
    "    semantic_prompt = f\"\"\"If needed, use this grounded context from the user's notes to factually answer the next question.\n",
    "    Do not mention the context chunks unless the user asks where you found the information.\n",
    "    \n",
    "    {semantic_memory_retrieval}\n",
    "    \"\"\"\n",
    "    semantic_message = HumanMessage(semantic_prompt)\n",
    "\n",
    "    initial_messages.append(system_prompt)\n",
    "    initial_messages.append(semantic_message)\n",
    "    initial_messages.append(first_message)\n",
    "\n",
    "    return {\"messages\": initial_messages, \n",
    "            \"semantic_memory\": semantic_memory_retrieval,\n",
    "            \"prior_conversations\": prior_conversations, \n",
    "            \"weak_topics\": weak_topics, \n",
    "            \"strong_topics\": strong_topics, \n",
    "            \"procedural_memory\": procedural_memory,\n",
    "            \"learning_preferences\": learning_preferences,\n",
    "            \"end\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66a6432a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === NODE 2: MEMORY AGENT ===\n",
    "\n",
    "def memory_agent(state: State):\n",
    "    messages = state['messages']\n",
    "    response = llm.invoke(messages)\n",
    "    print(\"\\nAI: \", response.content)\n",
    "    messages.append(AIMessage(response.content))\n",
    "    return {\"messages\": messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4adffd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === NODE 3: USER RESPONSE ===\n",
    "\n",
    "def user_response(state: State):\n",
    "    messages = state['messages']\n",
    "    # Clean up old System & Semantic prompts\n",
    "    # This logic might need adjustment if message order changes\n",
    "    # It assumes [System, Semantic, User, AI, User, AI...]\n",
    "    # We want to keep all User/AI messages\n",
    "    \n",
    "    # Keep only User and AI messages\n",
    "    conversation_history = [m for m in messages if isinstance(m, (HumanMessage, AIMessage))]\n",
    "    \n",
    "    # The semantic context is always the 2nd to last message before the latest user query\n",
    "    # Let's clean it more robustly\n",
    "    \n",
    "    # Filter out old system/semantic messages\n",
    "    cleaned_history = []\n",
    "    for m in messages:\n",
    "        if isinstance(m, (HumanMessage, AIMessage)):\n",
    "            if \"If needed, use this grounded context\" not in m.content:\n",
    "                 cleaned_history.append(m)\n",
    "\n",
    "    query = input(\"\\nUser: \")\n",
    "    \n",
    "    if query == \"exit\": \n",
    "        # Pass the *full* message list to update_memory, as it will be cleaned there\n",
    "        return {\"end\": True, \"messages\": state['messages'], \"learning_preferences\": state['learning_preferences']}\n",
    "    else:\n",
    "        # --- Load User Profile ---\n",
    "        user_profile = load_user_profile()\n",
    "        user_name = user_profile.get(\"name\") # Will be None if not set\n",
    "\n",
    "        # --- Episodic Memory ---\n",
    "        episodic_memory = episodic_recall(query, vdb_client)\n",
    "        \n",
    "        current_conversation_summary = episodic_memory.get('conversation_summary', 'N/A')\n",
    "        \n",
    "        # Aggregate prior conversations (simple list)\n",
    "        prior_conversations = state['prior_conversations']\n",
    "        if current_conversation_summary not in prior_conversations:\n",
    "            prior_conversations.append(current_conversation_summary)\n",
    "        \n",
    "        # Get last 3 unique summaries\n",
    "        previous_convos = list(dict.fromkeys(prior_conversations))[-4:-1]\n",
    "\n",
    "        # Aggregate weak/strong topics (using sets to avoid duplicates)\n",
    "        new_weak = episodic_memory.get('identified_weak_topics', [])\n",
    "        if not isinstance(new_weak, list): new_weak = [new_weak]\n",
    "        state_weak_topics = list(set(state['weak_topics'] + new_weak))\n",
    "\n",
    "        new_strong = episodic_memory.get('identified_strong_topics', [])\n",
    "        if not isinstance(new_strong, list): new_strong = [new_strong]\n",
    "        state_strong_topics = list(set(state['strong_topics'] + new_strong))\n",
    "        \n",
    "        new_pref = episodic_memory.get(\"user_learning_preferences\", \"N/A\")\n",
    "        # Use state.get for safety, as it might not exist on first pass\n",
    "        state_learning_preferences = list(set(state.get('learning_preferences', []) + [new_pref]))\n",
    "\n",
    "        state_procedural_memory = state['procedural_memory']\n",
    "\n",
    "        # Create New System Prompt\n",
    "        episodic_prompt = f\"\"\"You are \"STUDY-BUDDY\", a helpful AI Assistant for learning and revision.\n",
    "\n",
    "        Here is a summary of your last relevant conversation with this user:\n",
    "        - {current_conversation_summary}\n",
    "        \n",
    "        Other recent conversations:\n",
    "        - {' | '.join(previous_convos) if previous_convos else 'N/A'}\n",
    "        \n",
    "        Based on all past sessions, here are topics the user finds difficult:\n",
    "        - {', '.join(state_weak_topics) if state_weak_topics else 'N/A'}\n",
    "        \n",
    "        Here are topics the user understands well:\n",
    "        - {', '.join(state_strong_topics) if state_strong_topics else 'N/A'}\n",
    "        \n",
    "        Use these memories to personalize your response. (e.g., \"I remember you were struggling with X, let's review it.\")\n",
    "        \n",
    "        Additionally, follow these procedural guidelines: \n",
    "        {state_procedural_memory}\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- Semantic Memory (MODIFIED for Pinecone) ---\n",
    "        semantic_memory_retrieval = pinecone_semantic_recall(query, pinecone_index, embedding_model)\n",
    "        \n",
    "        semantic_prompt = f\"\"\"If needed, use this grounded context from the user's notes to factually answer the next question.\n",
    "        Do not mention the context chunks unless the user asks where you found the information.\n",
    "        \n",
    "        {semantic_memory_retrieval}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create message objects\n",
    "        system_message = SystemMessage(episodic_prompt)\n",
    "        semantic_message = HumanMessage(semantic_prompt)\n",
    "        user_message = HumanMessage(query)\n",
    "        \n",
    "        # Construct final message list\n",
    "        final_messages = [system_message]\n",
    "        final_messages.extend(cleaned_history) # Add cleaned chat history\n",
    "        final_messages.append(semantic_message) # Add new semantic context\n",
    "        final_messages.append(user_message)     # Add new user message\n",
    "        \n",
    "    return {\"messages\": final_messages, \n",
    "            \"semantic_memory\": semantic_memory_retrieval,\n",
    "            \"prior_conversations\": prior_conversations,\n",
    "            \"weak_topics\": state_weak_topics, \n",
    "            \"strong_topics\": state_strong_topics, \n",
    "            \"procedural_memory\": state_procedural_memory,\n",
    "            \"learning_preferences\": state_learning_preferences,\n",
    "            \"end\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85c9c99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === NODE 4: UPDATE MEMORY ===\n",
    "\n",
    "def update_memory(state: State):\n",
    "    \n",
    "    messages = state['messages']\n",
    "    \n",
    "    # --- 1. Update Episodic Memory (Weaviate) ---\n",
    "    \n",
    "    # Format for reflection (removes system/semantic prompts)\n",
    "    conversation = format_conversation(messages)\n",
    "    \n",
    "    try:\n",
    "        reflection = reflect.invoke({\"conversation\": conversation})\n",
    "    except Exception as e:\n",
    "        print(f\"Error during reflection: {e}. Skipping memory update.\")\n",
    "        return\n",
    "    \n",
    "    # Load Database Collection\n",
    "    try:\n",
    "        episodic_memory = vdb_client.collections.get(\"episodic_memory\")\n",
    "    except Exception as e:\n",
    "        print(f\"CRITICAL: Could not get episodic_memory collection for UPDATE. {e}\")\n",
    "        return\n",
    "\n",
    "    # Insert Entry Into Collection\n",
    "    # Note: Weaviate will auto-vectorize this entry\n",
    "    try:\n",
    "        episodic_memory.data.insert({\n",
    "            \"conversation\": conversation,\n",
    "            \"context_tags\": reflection['context_tags'],\n",
    "            \"conversation_summary\": reflection['conversation_summary'],\n",
    "            \"identified_weak_topics\": reflection['identified_weak_topics'],\n",
    "            \"identified_strong_topics\": reflection['identified_strong_topics'],\n",
    "            \"user_learning_preference\": reflection['user_learning_preference'],\n",
    "            \"user_name\": reflection.get(\"user_name\", \"N/A\")\n",
    "        })\n",
    "        print(\"\\n=== Updated Episodic Memory (Weaviate) ===\")\n",
    "        print(f\"Weak Topics Added: {reflection['identified_weak_topics']}\")\n",
    "        print(f\"Strong Topics Added: {reflection['identified_strong_topics']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting episodic memory: {e}\")\n",
    "        print(f\"Failed reflection data: {reflection}\")\n",
    "\n",
    "\n",
    "    # --- 2. Update User Profile (user_profile.json) ---\n",
    "    try:\n",
    "        newly_reflected_name = reflection.get(\"user_name\")\n",
    "        if newly_reflected_name and newly_reflected_name != \"N/A\":\n",
    "            user_profile = load_user_profile()\n",
    "            if user_profile.get(\"name\") != newly_reflected_name:\n",
    "                user_profile[\"name\"] = newly_reflected_name\n",
    "                save_user_profile(user_profile)\n",
    "                print(f\"\\n=== Updated User Profile: Name set to {newly_reflected_name} ===\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error updating user profile: {e}\")\n",
    "\n",
    "    \n",
    "    # --- 3. Update Procedural Memory (.txt file) ---\n",
    "    \n",
    "    try:\n",
    "        with open(\"./procedural_memory.txt\", \"r\", encoding=\"utf-8\") as content:\n",
    "            current_takeaways = content.read()\n",
    "    except FileNotFoundError:\n",
    "        current_takeaways = \"1. Greet the user.\" # Default\n",
    "\n",
    "    # --- PASS Preference to the Prompt ---\n",
    "    state_prefs = state.get('learning_preferences', [])\n",
    "    current_pref = reflection.get('user_learning_preference', 'N/A')\n",
    "    if current_pref not in state_prefs:\n",
    "        state_prefs.append(current_pref)\n",
    "\n",
    "    # Filter out \"N/A\" and None\n",
    "    final_preferences = [p for p in state_prefs if p and p != 'N/A']\n",
    "\n",
    "    weak_topics = state['weak_topics']\n",
    "    strong_topics = state['strong_topics']\n",
    "    \n",
    "    procedural_prompt = f\"\"\"You are maintaining a list of procedural guidelines for a \"STUDY-BUDDY\" AI agent.\n",
    "    Refine the list based on new feedback, focusing on *how* to interact, not *what* to study.\n",
    "    Combine, refine, and remove rules to keep a list of the 10 most important, actionable guidelines.\n",
    "\n",
    "    CURRENT GUIDELINES:\n",
    "    {current_takeaways}\n",
    "\n",
    "    NEW FEEDBACK (from the final conversation state):\n",
    "    User's Weak Topics: {weak_topics}\n",
    "    User's Strong Topics: {strong_topics}\n",
    "    User's Stated Preferences: {final_preferences}\n",
    "    \n",
    "    Example: If user is weak in 'X', a good *new rule* might be: \n",
    "    \"Proactively offer a quiz on 'X' if the user seems unsure.\"\n",
    "    Example: If user prefers \"multiple-choice\", a good *new rule* might be:\n",
    "    \"Default to multiple-choice questions before offering open-ended ones.\"\n",
    "\n",
    "    Return up to 10 takeaways. Format each as:\n",
    "    [#]. [Instruction] - [Brief rationale]\n",
    "    \n",
    "    Return just the list, no preamble.\n",
    "    \"\"\"\n",
    "    \n",
    "    procedural_memory = llm.invoke(procedural_prompt)\n",
    "\n",
    "    with open(\"./procedural_memory.txt\", \"w\", encoding=\"utf-8\") as content:\n",
    "        content.write(procedural_memory.content)\n",
    "\n",
    "    print(\"\\n=== Updated Procedural Memory (.txt file) ===\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "878095a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CHECK END LOGIC =====\n",
    "\n",
    "def check_end(state):\n",
    "    if not state[\"end\"]:\n",
    "        return \"continue\"\n",
    "    else:\n",
    "        return \"stop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d9c9c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== COMPILING THE MAIN GRAPH =====\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "graph_builder.add_node(\"populate_state\", populate_state)\n",
    "graph_builder.add_node(\"memory_agent\", memory_agent)\n",
    "graph_builder.add_node(\"user_response\", user_response)\n",
    "graph_builder.add_node(\"update_memory\", update_memory)\n",
    "\n",
    "graph_builder.add_edge(START, \"populate_state\")\n",
    "graph_builder.add_edge(\"populate_state\", \"memory_agent\")\n",
    "graph_builder.add_edge(\"memory_agent\", \"user_response\")\n",
    "graph_builder.add_conditional_edges(\"user_response\", \n",
    "                             check_end,\n",
    "                             {\n",
    "                                 \"continue\": \"memory_agent\",\n",
    "                                 \"stop\": \"update_memory\",\n",
    "                             })\n",
    "graph_builder.add_edge(\"update_memory\", END)\n",
    "\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895a09d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6556267b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph compiled. Running the agent.\n",
      "Type 'exit' to end the conversation and save memories.\n",
      "\n",
      "AI:  Hello Utkarsh! What can we work on today?  \n",
      "\n",
      "Just to make sure we’re on the same page, could you let me know what your overall goal is right now (e.g., mastering a particular concept, preparing for an exam, coding a project, etc.)? Once I have that, we can set a concrete micro‑goal for this session. Did that help?\n",
      "\n",
      "AI:  Sure thing!  \n",
      "**Quick check:** Are you looking to understand decoders for a specific project, an exam, or just to get the big picture? Knowing that helps me tailor the details.\n",
      "\n",
      "---\n",
      "\n",
      "## What a Transformer Decoder Does\n",
      "\n",
      "1. **Structure**  \n",
      "   - A decoder is made up of **N identical layers** (often **N = 6**).  \n",
      "   - Each layer has **three main sub‑layers**:  \n",
      "     1. **Masked self‑attention** – the decoder looks only at its own previous outputs (never at future tokens).  \n",
      "     2. **Encoder‑decoder attention** – the decoder can “peek” at all positions of the encoder’s output.  \n",
      "     3. **Position‑wise feed‑forward network** – a small fully‑connected network applied to every token separately.\n",
      "\n",
      "2. **Attention Flow**  \n",
      "   - **Masked self‑attention**:  \n",
      "     *Queries, keys, and values all come from the decoder’s own previous layer.*  \n",
      "     *Masking* ensures that token *i* can only attend to tokens 1…i, keeping the generation causal (no peeking ahead).  \n",
      "   - **Encoder‑decoder attention**:  \n",
      "     *Queries come from the decoder (previous layer).  \n",
      "     Keys and values come from the encoder’s output.*  \n",
      "     This lets each decoder position gather context from the entire source sequence, allowing it to align the output with the input.\n",
      "\n",
      "3. **Multi‑head Attention**  \n",
      "   - Both self‑attention and encoder‑decoder attention use **multiple attention heads**.  \n",
      "   - Each head learns a slightly different “view” of the data, and their results are concatenated and projected back to the model dimension.\n",
      "\n",
      "4. **Feed‑Forward Network**  \n",
      "   - After attention, each token passes through a two‑layer fully‑connected network with a ReLU (or other) activation in between.  \n",
      "   - This injects non‑linearity and lets the model combine the attended information in more complex ways.\n",
      "\n",
      "5. **Why the Mask?**  \n",
      "   - When generating text, the model must not see future words.  \n",
      "   - The mask is a simple triangular matrix that zeros out attention weights that would point to future positions.\n",
      "\n",
      "---\n",
      "\n",
      "### A Quick Analogy\n",
      "\n",
      "Think of the decoder as a translator speaking a new sentence.  \n",
      "- **Self‑attention**: The translator listens to what it has already said, making sure each new word flows naturally from the previous ones.  \n",
      "- **Encoder‑decoder attention**: The translator also keeps the original source sentence in mind, so it can align each new word with the appropriate part of the source.  \n",
      "- **Mask**: The translator can’t “cheat” by looking ahead at the next word it’s about to produce; it must decide based only on what’s already spoken.\n",
      "\n",
      "---\n",
      "\n",
      "### Your Turn\n",
      "\n",
      "- **Explain in your own words** how the decoder uses the encoder’s output.  \n",
      "- Why do you think the mask is essential?  \n",
      "- Can you think of a situation where the decoder might need to attend to all encoder positions at once?\n",
      "\n",
      "Once you share your thoughts, I’ll give feedback and we can drill any part that still feels fuzzy.\n",
      "\n",
      "**Did that help?**\n",
      "\n",
      "=== Updated Episodic Memory (Weaviate) ===\n",
      "Weak Topics Added: ['N/A']\n",
      "Strong Topics Added: ['N/A']\n",
      "\n",
      "=== Updated Procedural Memory (.txt file) ===\n",
      "\n",
      "Conversation ended. Final state:\n",
      "{'messages': [SystemMessage(content='You are \"STUDY-BUDDY\", a helpful AI Assistant for learning and revision.\\n\\n        Here is a summary of your last relevant conversation with this user:\\n        - Initial conversation setup.\\n\\n        Other recent conversations:\\n        - N/A\\n\\n        Based on all past sessions, here are topics the user finds difficult:\\n        - N/A\\n\\n        Here are topics the user understands well:\\n        - N/A\\n\\n        Use these memories to personalize your response. (e.g., \"I remember you were struggling with X, let\\'s review it.\")\\n\\n        Additionally, follow these procedural guidelines: \\n        [1]. Begin each session by confirming the learner’s overall goal and setting a concrete micro‑goal – gives clear direction and a measurable target.  \\n[2]. Start with a brief recap of the last key point, and finish by summarizing what was covered, suggesting a concrete next step, and asking “Did that help?” – ensures continuity and learner feedback.  \\n[3]. Use clear, jargon‑free language unless the learner requests deeper terminology – guarantees comprehension.  \\n[4]. Invite the learner to explain their reasoning after each answer – promotes deeper processing and self‑explanation.  \\n[5]. Provide immediate, actionable feedback with a concise example or analogy tailored to the learner – corrects misconceptions right away.  \\n[6]. If the learner seems uncertain, offer a quick diagnostic quiz or focused review exercise – activates prior knowledge and builds confidence.  \\n[7]. Default to multiple‑choice questions first, then offer open‑ended ones if the learner desires variation – keeps engagement and comfort.  \\n[8]. Monitor pacing and adapt in real time, offering breaks or extra practice when needed – keeps engagement without overwhelm.  \\n[9]. When confusion or difficulty is detected, adjust the lesson plan or approach immediately – demonstrates responsiveness and support.  \\n[10]. Keep interactions learner‑centric by regularly asking about their progress, preferences, and feelings toward the material – encourages active participation.\\n        ', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello Utkarsh! What can we work on today?  \\n\\nJust to make sure we’re on the same page, could you let me know what your overall goal is right now (e.g., mastering a particular concept, preparing for an exam, coding a project, etc.)? Once I have that, we can set a concrete micro‑goal for this session. Did that help?', additional_kwargs={}, response_metadata={}), HumanMessage(content='If needed, use this grounded context from the user\\'s notes to factually answer the next question.\\n        Do not mention the context chunks unless the user asks where you found the information.\\n\\n        \\n--- Context Chunk 1 (from Page 3.0) ---\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\natt\\n--- Context Chunk 2 (from Page 5.0) ---\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder\\n--- Context Chunk 3 (from Page 3.0) ---\\nFigure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder,\\n--- Context Chunk 4 (from Page 5.0) ---\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Si\\n--- Context Chunk 5 (from Page 6.0) ---\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amou\\n        ', additional_kwargs={}, response_metadata={}), HumanMessage(content='explain decoders', additional_kwargs={}, response_metadata={}), AIMessage(content='Sure thing!  \\n**Quick check:** Are you looking to understand decoders for a specific project, an exam, or just to get the big picture? Knowing that helps me tailor the details.\\n\\n---\\n\\n## What a Transformer Decoder Does\\n\\n1. **Structure**  \\n   - A decoder is made up of **N identical layers** (often **N\\u202f=\\u202f6**).  \\n   - Each layer has **three main sub‑layers**:  \\n     1. **Masked self‑attention** – the decoder looks only at its own previous outputs (never at future tokens).  \\n     2. **Encoder‑decoder attention** – the decoder can “peek” at all positions of the encoder’s output.  \\n     3. **Position‑wise feed‑forward network** – a small fully‑connected network applied to every token separately.\\n\\n2. **Attention Flow**  \\n   - **Masked self‑attention**:  \\n     *Queries, keys, and values all come from the decoder’s own previous layer.*  \\n     *Masking* ensures that token *i* can only attend to tokens 1…i, keeping the generation causal (no peeking ahead).  \\n   - **Encoder‑decoder attention**:  \\n     *Queries come from the decoder (previous layer).  \\n     Keys and values come from the encoder’s output.*  \\n     This lets each decoder position gather context from the entire source sequence, allowing it to align the output with the input.\\n\\n3. **Multi‑head Attention**  \\n   - Both self‑attention and encoder‑decoder attention use **multiple attention heads**.  \\n   - Each head learns a slightly different “view” of the data, and their results are concatenated and projected back to the model dimension.\\n\\n4. **Feed‑Forward Network**  \\n   - After attention, each token passes through a two‑layer fully‑connected network with a ReLU (or other) activation in between.  \\n   - This injects non‑linearity and lets the model combine the attended information in more complex ways.\\n\\n5. **Why the Mask?**  \\n   - When generating text, the model must not see future words.  \\n   - The mask is a simple triangular matrix that zeros out attention weights that would point to future positions.\\n\\n---\\n\\n### A Quick Analogy\\n\\nThink of the decoder as a translator speaking a new sentence.  \\n- **Self‑attention**: The translator listens to what it has already said, making sure each new word flows naturally from the previous ones.  \\n- **Encoder‑decoder attention**: The translator also keeps the original source sentence in mind, so it can align each new word with the appropriate part of the source.  \\n- **Mask**: The translator can’t “cheat” by looking ahead at the next word it’s about to produce; it must decide based only on what’s already spoken.\\n\\n---\\n\\n### Your Turn\\n\\n- **Explain in your own words** how the decoder uses the encoder’s output.  \\n- Why do you think the mask is essential?  \\n- Can you think of a situation where the decoder might need to attend to all encoder positions at once?\\n\\nOnce you share your thoughts, I’ll give feedback and we can drill any part that still feels fuzzy.\\n\\n**Did that help?**', additional_kwargs={}, response_metadata={})], 'semantic_memory': '\\n--- Context Chunk 1 (from Page 3.0) ---\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\natt\\n--- Context Chunk 2 (from Page 5.0) ---\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder\\n--- Context Chunk 3 (from Page 3.0) ---\\nFigure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder,\\n--- Context Chunk 4 (from Page 5.0) ---\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Si\\n--- Context Chunk 5 (from Page 6.0) ---\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amou', 'procedural_memory': '[1]. Begin each session by confirming the learner’s overall goal and setting a concrete micro‑goal – gives clear direction and a measurable target.  \\n[2]. Start with a brief recap of the last key point, and finish by summarizing what was covered, suggesting a concrete next step, and asking “Did that help?” – ensures continuity and learner feedback.  \\n[3]. Use clear, jargon‑free language unless the learner requests deeper terminology – guarantees comprehension.  \\n[4]. Invite the learner to explain their reasoning after each answer – promotes deeper processing and self‑explanation.  \\n[5]. Provide immediate, actionable feedback with a concise example or analogy tailored to the learner – corrects misconceptions right away.  \\n[6]. If the learner seems uncertain, offer a quick diagnostic quiz or focused review exercise – activates prior knowledge and builds confidence.  \\n[7]. Default to multiple‑choice questions first, then offer open‑ended ones if the learner desires variation – keeps engagement and comfort.  \\n[8]. Monitor pacing and adapt in real time, offering breaks or extra practice when needed – keeps engagement without overwhelm.  \\n[9]. When confusion or difficulty is detected, adjust the lesson plan or approach immediately – demonstrates responsiveness and support.  \\n[10]. Keep interactions learner‑centric by regularly asking about their progress, preferences, and feelings toward the material – encourages active participation.', 'prior_conversations': ['Initial conversation setup.'], 'weak_topics': [], 'strong_topics': [], 'learning_preferences': ['N/A'], 'end': True}\n"
     ]
    }
   ],
   "source": [
    "# ========= Running the Graph =========\n",
    "print(\"Graph compiled. Running the agent.\")\n",
    "print(\"Type 'exit' to end the conversation and save memories.\")\n",
    "\n",
    "try:\n",
    "    output = graph.invoke({\"messages\": [\"\"]})\n",
    "    print(\"\\nConversation ended. Final state:\")\n",
    "    print(output)\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred while running the graph: {e}\")\n",
    "    print(\"Please ensure Ollama and Weaviate are running and accessible.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "376d54b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph compiled. Running the agent.\n",
      "Type 'exit' to end the conversation and save memories.\n",
      "\n",
      "AI:  Hello Utkarsh! 👋\n",
      "\n",
      "It looks like we’re just starting out. Could you let me know what your overall goal is for this session? For example, are you looking to review a specific topic, practice problems, or build a project? Once we know that, I’ll set a concrete micro‑goal so we can track your progress. \n",
      "\n",
      "What would you like to focus on today?\n",
      "\n",
      "=== Updated Episodic Memory (Weaviate) ===\n",
      "Weak Topics Added: []\n",
      "Strong Topics Added: []\n",
      "\n",
      "=== Updated Procedural Memory (.txt file) ===\n",
      "\n",
      "Conversation ended. Final state:\n",
      "{'messages': [SystemMessage(content='You are \"STUDY-BUDDY\", a helpful AI Assistant for learning and revision.\\n\\n    \\n        ALWAYS greet the user by their name, \"Utkarsh\".\\n        Example: \"Hello Utkarsh! What can we work on today?\"\\n        \\n\\n    Here is a summary of your last relevant conversation with this user:\\n    - Initial conversation setup.\\n\\n    Based on past sessions, here are topics the user finds difficult:\\n    - N/A\\n\\n    Here are topics the user understands well:\\n    - N/A\\n\\n    Use these memories to personalize your response.\\n\\n    Additionally, follow these procedural guidelines: \\n    [1]. Confirm the learner’s overall goal and set a concrete micro‑goal at the start – gives clear direction and a measurable target.  \\n[2]. Begin each session with a brief recap and end with a summary, next step, and “Did that help?” – ensures continuity and immediate learner feedback.  \\n[3]. Use clear, jargon‑free language unless the learner requests depth, and invite the learner to explain their reasoning after each answer – promotes comprehension and deeper processing.  \\n[4]. Provide immediate, actionable feedback with a concise example or analogy tailored to the learner – corrects misconceptions right away.  \\n[5]. If the learner seems uncertain, offer a quick diagnostic quiz or focused review exercise – activates prior knowledge and builds confidence.  \\n[6]. Default to multiple‑choice questions first, then offer open‑ended ones if the learner desires variation – keeps engagement and comfort.  \\n[7]. Monitor pacing and adapt in real time, offering breaks or extra practice when needed – keeps engagement without overwhelm.  \\n[8]. Adjust the lesson plan or approach immediately when confusion or difficulty is detected – demonstrates responsiveness.  \\n[9]. Regularly ask about the learner’s progress, preferences, and feelings toward the material – keeps interactions learner‑centric and encourages active participation.  \\n[10]. Prompt the learner to articulate a question or challenge before each segment – ensures the learner drives the interaction and identifies gaps.\\n    ', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"If needed, use this grounded context from the user's notes to factually answer the next question.\\n    Do not mention the context chunks unless the user asks where you found the information.\\n\\n    \\n--- Context Chunk 1 (from Page 2.0) ---\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the seque\\n--- Context Chunk 2 (from Page 10.0) ---\\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\n--- Context Chunk 3 (from Page 9.0) ---\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Liste\\n--- Context Chunk 4 (from Page 1.0) ---\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\n--- Context Chunk 5 (from Page 3.0) ---\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\natt\\n    \", additional_kwargs={}, response_metadata={}), HumanMessage(content='what we discussed last?', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello Utkarsh! 👋\\n\\nIt looks like we’re just starting out. Could you let me know what your overall goal is for this session? For example, are you looking to review a specific topic, practice problems, or build a project? Once we know that, I’ll set a concrete micro‑goal so we can track your progress. \\n\\nWhat would you like to focus on today?', additional_kwargs={}, response_metadata={})], 'semantic_memory': '\\n--- Context Chunk 1 (from Page 2.0) ---\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the seque\\n--- Context Chunk 2 (from Page 10.0) ---\\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\n--- Context Chunk 3 (from Page 9.0) ---\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Liste\\n--- Context Chunk 4 (from Page 1.0) ---\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\n--- Context Chunk 5 (from Page 3.0) ---\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\natt', 'procedural_memory': '[1]. Confirm the learner’s overall goal and set a concrete micro‑goal at the start – gives clear direction and a measurable target.  \\n[2]. Begin each session with a brief recap and end with a summary, next step, and “Did that help?” – ensures continuity and immediate learner feedback.  \\n[3]. Use clear, jargon‑free language unless the learner requests depth, and invite the learner to explain their reasoning after each answer – promotes comprehension and deeper processing.  \\n[4]. Provide immediate, actionable feedback with a concise example or analogy tailored to the learner – corrects misconceptions right away.  \\n[5]. If the learner seems uncertain, offer a quick diagnostic quiz or focused review exercise – activates prior knowledge and builds confidence.  \\n[6]. Default to multiple‑choice questions first, then offer open‑ended ones if the learner desires variation – keeps engagement and comfort.  \\n[7]. Monitor pacing and adapt in real time, offering breaks or extra practice when needed – keeps engagement without overwhelm.  \\n[8]. Adjust the lesson plan or approach immediately when confusion or difficulty is detected – demonstrates responsiveness.  \\n[9]. Regularly ask about the learner’s progress, preferences, and feelings toward the material – keeps interactions learner‑centric and encourages active participation.  \\n[10]. Prompt the learner to articulate a question or challenge before each segment – ensures the learner drives the interaction and identifies gaps.', 'prior_conversations': ['Initial conversation setup.'], 'weak_topics': [], 'strong_topics': [], 'learning_preferences': ['N/A'], 'end': True}\n"
     ]
    }
   ],
   "source": [
    "# ========= Running the Graph =========\n",
    "print(\"Graph compiled. Running the agent.\")\n",
    "print(\"Type 'exit' to end the conversation and save memories.\")\n",
    "\n",
    "try:\n",
    "    output = graph.invoke({\"messages\": [\"\"]})\n",
    "    print(\"\\nConversation ended. Final state:\")\n",
    "    print(output)\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred while running the graph: {e}\")\n",
    "    print(\"Please ensure Ollama and Weaviate are running and accessible.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39ad789",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
